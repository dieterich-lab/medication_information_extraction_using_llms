{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/homes/prichter/venv_unsloth/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import dspy\n",
    "import ast\n",
    "import tqdm\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import List, Tuple, Set, Dict\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import difflib\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_counter(name):\n",
    "    \"\"\"\n",
    "    Extracts a counter (n) at the end of the medication.\n",
    "    Returns the medication name (without counter) and the counter.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\s*\\((\\d+)\\)\\s*$', name)\n",
    "    if match:\n",
    "        counter = match.group(1)\n",
    "        name_without_counter = name[:match.start()].strip()\n",
    "        return name_without_counter, counter\n",
    "    else:\n",
    "        return name.strip(), None\n",
    "\n",
    "def preprocess_med_name(name):\n",
    "    \"\"\"\n",
    "    Preprocess medication name:\n",
    "    - Extracting counter\n",
    "    - Lowercase\n",
    "    - Remove any content in parentheses (except the counter)\n",
    "    - Remove any punctuation except hyphens and spaces\n",
    "    - Remove spaces\n",
    "    \"\"\"\n",
    "    name_without_counter, counter = extract_counter(name)\n",
    "    name_clean = name_without_counter.lower()\n",
    "    name_clean = re.sub(r'\\([^)]*\\)', '', name_clean)\n",
    "    name_clean = re.sub(r'[^\\w\\s-]', '', name_clean)\n",
    "    name_clean = ' '.join(name_clean.split())\n",
    "    \n",
    "    return name_clean, counter\n",
    "\n",
    "def are_med_names_matching(name1, name2, threshold):\n",
    "    \"\"\"\n",
    "    Compare two medication names\n",
    "    \n",
    "    Parameters:\n",
    "    - med name 1, med name 2\n",
    "    - threshold: similarity score threshold\n",
    "    \n",
    "    Returns:\n",
    "    - True if match, False otherwise.\n",
    "    \"\"\"\n",
    "    name1_clean, counter1 = preprocess_med_name(name1)\n",
    "    name2_clean, counter2 = preprocess_med_name(name2)\n",
    "    \n",
    "    name1_clean = re.sub(r'\\b(\\w+)\\s+(n|en|e)\\b', r'\\1\\2', name1_clean)\n",
    "    name2_clean = re.sub(r'\\b(\\w+)\\s+(n|en|e)\\b', r'\\1\\2', name2_clean)\n",
    "    \n",
    "    # Check if the first characters are the same\n",
    "    if not name1_clean or not name2_clean:\n",
    "        return False, 0.0\n",
    "    if name1_clean[0] != name2_clean[0]:\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Similarity score of the medication names\n",
    "    ratio = difflib.SequenceMatcher(None, name1_clean, name2_clean).ratio()\n",
    "    \n",
    "    # If counters are different => not a match\n",
    "    if counter1 != counter2:\n",
    "        return False, ratio\n",
    "    \n",
    "    # If ratio >= threshold it is a match\n",
    "    return ratio >= threshold, ratio\n",
    "\n",
    "def adapt_medication_names(gold_dict, pred_dict, threshold):\n",
    "    \"\"\"\n",
    "    Compare medication names. If match align med name in pred and gold.\n",
    "    \n",
    "    Parameters:\n",
    "    - gold_dict: medication gold dict\n",
    "    - pred_dict: medication pred dict\n",
    "    - threshold: similarity score\n",
    "    \n",
    "    Returns:\n",
    "    - medication pred dict with aligned medication dict\n",
    "    \"\"\"\n",
    "    gold_meds = gold_dict.get('medications', [])\n",
    "    pred_meds = pred_dict.get('medications', [])\n",
    "    \n",
    "    # Check each medication in pred_dict\n",
    "    for pred_med in pred_meds:\n",
    "        pred_name = pred_med.get('medication', '')\n",
    "        best_match_name = None\n",
    "        highest_ratio = 0\n",
    "        \n",
    "        # Compare with each medication in gold_dict\n",
    "        for gold_med in gold_meds:\n",
    "            gold_name = gold_med.get('medication', '')\n",
    "            match, ratio = are_med_names_matching(pred_name, gold_name, threshold)\n",
    "            if match and ratio > highest_ratio:\n",
    "                highest_ratio = ratio\n",
    "                best_match_name = gold_name\n",
    "        \n",
    "        # If match is found, update the pred_med 'medication' name\n",
    "        if best_match_name:\n",
    "            pred_med['medication'] = best_match_name\n",
    "    \n",
    "    return pred_dict\n",
    "\n",
    "def reorder_dict(gold_samples, pred_samples):\n",
    "    \"\"\"\n",
    "    Order pred dict similar to gold_dict. in-place\n",
    "    \n",
    "    Parameters:\n",
    "    - gold_dict: medication gold dict\n",
    "    - pred_dict: medication pred dict\n",
    "    \"\"\"\n",
    "    for gold_sample, pred_sample in zip(gold_samples, pred_samples):\n",
    "        gold_dict = gold_sample[1]\n",
    "        pred_dict = pred_sample[1]\n",
    "\n",
    "        if len(gold_dict['medications']) > 0:\n",
    "            first_keys_order = list(gold_dict['medications'][0].keys())\n",
    "        \n",
    "            for i, med in enumerate(pred_dict['medications']):\n",
    "                sorted_med = OrderedDict((key, med[key]) for key in first_keys_order)            \n",
    "                pred_dict['medications'][i] = sorted_med\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def ensure_keys_in_medications(data):\n",
    "    \"\"\"\n",
    "    Check, if all keys (relation information) is present. in-place.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: gold or pred dict\n",
    "    \"\"\"\n",
    "    RE_CLASSES_inclmed = RE_CLASSES.union({'medication'})\n",
    "    count = 0\n",
    "    for entry in data:\n",
    "        text, ordered_dict = entry\n",
    "        medications_list = ordered_dict.get('medications', [])\n",
    "        \n",
    "        for medication_dict in medications_list:\n",
    "            # Check for each required key, add if missing with an empty value\n",
    "            for key in RE_CLASSES_inclmed:\n",
    "                if key not in medication_dict:\n",
    "                    try:\n",
    "                        medication_dict[key] = ''  # Add missing key with an empty value\n",
    "                    except Exception as e:\n",
    "                        print(entry)\n",
    "                        count += 1\n",
    "                        print(f\"Error: {e} - {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(gold_samples: List[Tuple[str, OrderedDict]], pred_samples: List[Tuple[str, OrderedDict]], use_medication_threshold: bool) -> Tuple[List[Tuple[str, str, str, str]], List[Tuple[str, str, str, str]]]:\n",
    "    \"\"\"\n",
    "    Extract relations from a list of samples. Ignore 'no drugs' sample\n",
    "\n",
    "    Parameters:\n",
    "    - gold_samples: List of gold samples\n",
    "    - pred_samples: List of pred samples\n",
    "\n",
    "    Returns:\n",
    "    - Tuples for gold and predicted samples.\n",
    "    \"\"\"\n",
    "        \n",
    "    gold_triples = []\n",
    "    pred_triples = []\n",
    "\n",
    "    ensure_keys_in_medications(gold_samples)\n",
    "    ensure_keys_in_medications(pred_samples)\n",
    "\n",
    "    reorder_dict(gold_samples, pred_samples)\n",
    "    \n",
    "    for gold, pred in zip(gold_samples, pred_samples):\n",
    "        gold_text, gold_dict = gold\n",
    "        pred_text, pred_dict = pred\n",
    "\n",
    "        # Ensure pred and gold texts are the same\n",
    "        assert gold_text == pred_text, \"Mismatch in texts between gold and pred.\"\n",
    "        gold_medications = gold_dict.get('medications', [])\n",
    "        pred_medications = pred_dict.get('medications', [])\n",
    "\n",
    "        if use_medication_threshold:\n",
    "            pred_dict = adapt_medication_names(gold_dict, pred_dict, threshold=0.75)\n",
    "\n",
    "        # Extract medication information from gold and pred\n",
    "        gold_medications = gold_dict.get('medications', [])\n",
    "        pred_medications = pred_dict.get('medications', [])\n",
    "        \n",
    "        # Convert list of medication dictionaries to lists with counts for comparison\n",
    "        gold_drugs_counts = defaultdict(int)  \n",
    "        pred_drugs_counts = defaultdict(int)  \n",
    "\n",
    "        for med in gold_medications:  \n",
    "            gold_drugs_counts[med['medication']] += 1  \n",
    "\n",
    "        for med in pred_medications:  \n",
    "            pred_drugs_counts[med['medication']] += 1\n",
    "\n",
    "        # Identify drugs with counts in both gold and predicted samples\n",
    "        common_drugs = set(gold_drugs_counts.keys()) & set(pred_drugs_counts.keys())\n",
    "        gold_only = set(gold_drugs_counts.keys()) - set(pred_drugs_counts.keys())\n",
    "        pred_only = set(pred_drugs_counts.keys()) - set(gold_drugs_counts.keys())\n",
    "\n",
    "        # Process drugs\n",
    "        for drug in common_drugs:\n",
    "            # Extract the correct number of instances for each drug\n",
    "            gold_props_list = [med for med in gold_medications if med['medication'] == drug]  \n",
    "            pred_props_list = [med for med in pred_medications if med['medication'] == drug]  \n",
    "\n",
    "            gold_instances_count = gold_drugs_counts[drug]  \n",
    "            pred_instances_count = pred_drugs_counts[drug]  \n",
    "            \n",
    "            common_count = min(gold_instances_count, pred_instances_count)  \n",
    "            gold_props_list = gold_props_list[:common_count]  \n",
    "            pred_props_list = pred_props_list[:common_count]\n",
    "\n",
    "            for gold_props in gold_props_list:\n",
    "                for rel_class, rel_value in gold_props.items():\n",
    "                    if rel_class != 'medication':\n",
    "                        gold_triples.append((gold_text, drug, rel_class, rel_value))\n",
    "\n",
    "            for pred_props in pred_props_list:\n",
    "                for rel_class, rel_value in pred_props.items():\n",
    "                    if rel_class != 'medication':\n",
    "                        pred_triples.append((pred_text, drug, rel_class, rel_value))\n",
    "        \n",
    "        # Process gold-only drugs\n",
    "        for drug in gold_only:\n",
    "            gold_props_list = [med for med in gold_medications if med['medication'] == drug]\n",
    "            for gold_props in gold_props_list:\n",
    "                for rel_class, rel_value in gold_props.items():\n",
    "                    if rel_class != 'medication':\n",
    "                        gold_triples.append((gold_text, drug, rel_class, rel_value))\n",
    "                        pred_triples.append((pred_text, 'no drugs', rel_class, ''))\n",
    "        \n",
    "        # Process pred-only drugs\n",
    "        for drug in pred_only:\n",
    "            pred_props_list = [med for med in pred_medications if med['medication'] == drug]\n",
    "            for pred_props in pred_props_list:\n",
    "                for rel_class, rel_value in pred_props.items():\n",
    "                    if rel_class != 'medication':\n",
    "                        pred_triples.append((pred_text, drug, rel_class, rel_value))\n",
    "                        gold_triples.append((gold_text, \"no drugs\", rel_class, ''))\n",
    "\n",
    "    return gold_triples, pred_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_re_metrics(gold_relations: List[Tuple[str, str, str, str]], pred_relations: List[Tuple[str, str, str, str]], classes: Set[str], complete: bool, target_class: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Compute exact and lenient TP, FP, FN for each relation class.\n",
    "\n",
    "    Parameters:\n",
    "    - gold_relations: list of gold (text, drug, class, value) tuples.\n",
    "    - pred_relations: list of pred (text, drug, class, value) tuples.\n",
    "    - classes: set of relation classes to evaluate.\n",
    "    - complete: whether to use the complete matching strategy. Meaning: if gold_drug != pred_drug => model failed to identify drug. \n",
    "    Then all relation values are counted as FN and all pred_drug relation values are FP.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with class as key and metrics as value.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {cls: {\n",
    "        'Exact_TP': 0,\n",
    "        'Exact_FP': 0,\n",
    "        'Exact_FN': 0,\n",
    "        'Exact_TN': 0,\n",
    "        'Any_TP': 0,\n",
    "        'Any_FP': 0,\n",
    "        'Any_FN': 0,\n",
    "        'Any_TN': 0,\n",
    "        'Lenient_TP': 0,\n",
    "        'Lenient_FP': 0,\n",
    "        'Lenient_FN': 0,\n",
    "        'Support': 0 \n",
    "        } for cls in classes}\n",
    "\n",
    "    false_positives = {key: [] for key in classes}\n",
    "    false_negatives = {key: [] for key in classes}\n",
    "\n",
    "    # Normalize relation values\n",
    "    def normalize_relation_value(value, relation_class):\n",
    "        def normalize_word(word):\n",
    "            word = re.sub(r'\\.', '', word.strip().lower())            \n",
    "            word = re.sub(r'\\s*\\(s\\)\\s*', '', word)\n",
    "            return word.strip()\n",
    "    \n",
    "        # If input is string, normalize the string\n",
    "        if isinstance(value, str):\n",
    "            value = normalize_word(value)\n",
    "            return value\n",
    "    \n",
    "        # If input is list, normalize each element in the list\n",
    "        elif isinstance(value, list):\n",
    "            value = [normalize_word(v) for v in value]\n",
    "            return value\n",
    "    \n",
    "        # If the input is neither string nor list, return it as is\n",
    "        else:\n",
    "            return value\n",
    "        \n",
    "    def normalize_drug_name(drug_name):\n",
    "        # lowercase\n",
    "        drug_name = drug_name.lower()\n",
    "        # remove leading/trailing whitespace\n",
    "        drug_name = drug_name.strip()\n",
    "        # replace hyphens with a space\n",
    "        drug_name = drug_name.replace('-', ' ')\n",
    "        # remove special characters\n",
    "        drug_name = re.sub(r'[^\\w\\s]', '', drug_name)\n",
    "        # replace multiple spaces with single space\n",
    "        drug_name = re.sub(r'\\s+', ' ', drug_name)\n",
    "        # remove all whitespace and special characters\n",
    "        drug_name = re.sub(r'\\W+', '', drug_name)\n",
    "        return drug_name\n",
    "\n",
    "    for index in range(len(gold_relations)):\n",
    "        gold_sample = gold_relations[index]\n",
    "        pred_sample = pred_relations[index]\n",
    "        \n",
    "        text, gold_drug, gold_relation_class, gold_relation_value = gold_sample\n",
    "        text, pred_drug, pred_relation_class, pred_relation_value = pred_sample\n",
    "    \n",
    "        gold_relation_value = normalize_relation_value(gold_relation_value, gold_relation_class)\n",
    "        pred_relation_value = normalize_relation_value(pred_relation_value, gold_relation_class) \n",
    "\n",
    "        gold_drug = normalize_drug_name(gold_drug)\n",
    "        pred_drug = normalize_drug_name(pred_drug)       \n",
    "    \n",
    "        relation_class = gold_relation_class\n",
    "    \n",
    "        # Check if relation values are lists for consistent processing\n",
    "        gold_values = gold_relation_value if isinstance(gold_relation_value, list) else [gold_relation_value] if gold_relation_value else []\n",
    "        pred_values = pred_relation_value if isinstance(pred_relation_value, list) else [pred_relation_value] if pred_relation_value else []\n",
    "        \n",
    "        # DEBUGGING: Avoid typos for small 8b zero shot models\n",
    "        #if relation_class == 'dosagemg' or relation_class == 'dosagem':\n",
    "        #    relation_class = 'dosage'\n",
    "        #if relation_class == 'refills':\n",
    "        #    continue#break\n",
    "        #if relation_class == 'Ant':\n",
    "        #    continue#break\n",
    "            \n",
    "        # Update support value for the class\n",
    "        metrics[relation_class]['Support'] += len(gold_values)\n",
    "\n",
    "        # Initialize counts and matched indices for this sample\n",
    "        exact_tp = 0\n",
    "        lenient_tp = 0\n",
    "        exact_fp = 0\n",
    "        lenient_fp = 0\n",
    "        exact_fn = 0\n",
    "        lenient_fn = 0\n",
    "        \n",
    "        exact_matched_gold_indices = []\n",
    "        exact_matched_pred_indices = []\n",
    "        lenient_matched_gold_indices = []\n",
    "        lenient_matched_pred_indices = []\n",
    "\n",
    "        if gold_drug == pred_drug:\n",
    "            # Compare gold and predicted values for exact match\n",
    "            for gold_idx, gold_value in enumerate(gold_values):\n",
    "                exact_match_found = False\n",
    "                lenient_match_found = False\n",
    "                for pred_idx, pred_value in enumerate(pred_values):\n",
    "                    if gold_value == pred_value and pred_idx not in exact_matched_pred_indices:\n",
    "                        # Exact match{}\n",
    "                        exact_tp += 1\n",
    "                        exact_matched_gold_indices.append(gold_idx)\n",
    "                        exact_matched_pred_indices.append(pred_idx)\n",
    "                        # Also count exact as lenient match\n",
    "                        lenient_tp += 1\n",
    "                        lenient_matched_gold_indices.append(gold_idx)\n",
    "                        lenient_matched_pred_indices.append(pred_idx)\n",
    "                        exact_match_found = True\n",
    "                        break\n",
    "                # If no exact match, check for lenient match\n",
    "                if not exact_match_found:\n",
    "                    for pred_idx, pred_value in enumerate(pred_values):\n",
    "                        if pred_idx not in lenient_matched_pred_indices:\n",
    "                            gold_words = gold_value.split()\n",
    "                            pred_words = pred_value.split()\n",
    "                            gold_words = [word.strip(string.punctuation).lower() for word in gold_words]\n",
    "                            pred_words = [word.strip(string.punctuation).lower() for word in pred_words]\n",
    "                            if any(word in pred_words for word in gold_words):\n",
    "                                lenient_tp += 1\n",
    "                                lenient_matched_gold_indices.append(gold_idx)\n",
    "                                lenient_matched_pred_indices.append(pred_idx)\n",
    "                                lenient_match_found = True\n",
    "                                break\n",
    "                    # Count exact FN if no exact match was found, regardless of lenient match\n",
    "                    exact_fn += 1\n",
    "                    if not lenient_match_found:\n",
    "                        # Increment Lenient_FN if no lenient match was found\n",
    "                        lenient_fn += 1\n",
    "        \n",
    "            # Any unmatched predicted values are false positives\n",
    "            for pred_idx, pred_value in enumerate(pred_values):\n",
    "                if pred_idx not in exact_matched_pred_indices:\n",
    "                    exact_fp += 1\n",
    "                if pred_idx not in lenient_matched_pred_indices:\n",
    "                    lenient_fp += 1\n",
    "        \n",
    "        else:\n",
    "            if complete:\n",
    "                # Drug names do not match; count all gold values as FN and predicted values as FP\n",
    "                exact_fn += len(gold_values)\n",
    "                lenient_fn += len(gold_values)\n",
    "                exact_fp += len(pred_values)\n",
    "                lenient_fp += len(pred_values)\n",
    "            else:\n",
    "                # Drug names do not match; count only per missed drug name as FN and predicted values as FP             \n",
    "                exact_fn += 1 if len(gold_values) >= 1 else 0 #len(gold_values)\n",
    "                lenient_fn += 1 if len(gold_values) >= 1 else 0 #len(gold_values)\n",
    "                exact_fp += 1 if len(pred_values) >= 1 else 0 #len(pred_values)\n",
    "                lenient_fp += 1 if len(pred_values) >= 1 else 0 #len(pred_values)                \n",
    "    \n",
    "        # Update metrics\n",
    "        metrics[relation_class]['Exact_TP'] += exact_tp\n",
    "        metrics[relation_class]['Exact_FP'] += exact_fp\n",
    "        metrics[relation_class]['Exact_FN'] += exact_fn\n",
    "        \n",
    "        metrics[relation_class]['Lenient_TP'] += lenient_tp\n",
    "        metrics[relation_class]['Lenient_FP'] += lenient_fp\n",
    "        metrics[relation_class]['Lenient_FN'] += lenient_fn        \n",
    "               \n",
    "        # Handle 'Any' metrics\n",
    "        if relation_class in RE_CLASSES:\n",
    "            any_match_found = False\n",
    "            if gold_values and pred_values:\n",
    "                for gold_value in gold_values:\n",
    "                    gold_words = [word.strip(string.punctuation).lower() for word in gold_value.split()]\n",
    "                    for pred_value in pred_values:\n",
    "                        pred_words = [word.strip(string.punctuation).lower() for word in pred_value.split()]\n",
    "                        if any(word in pred_words for word in gold_words):\n",
    "                            any_match_found = True\n",
    "                            break\n",
    "                    if any_match_found:\n",
    "                        break\n",
    "                if any_match_found:\n",
    "                    metrics[relation_class]['Any_TP'] += 1\n",
    "                else:\n",
    "                    metrics[relation_class]['Any_FN'] += 1\n",
    "            elif gold_values and not pred_values:\n",
    "                metrics[relation_class]['Any_FN'] += 1\n",
    "            elif not gold_values and pred_values:\n",
    "                metrics[relation_class]['Any_FP'] += 1\n",
    "\n",
    "        # Collect FP\n",
    "        for pred_idx, pred_value in enumerate(pred_values):\n",
    "            if pred_idx not in lenient_matched_pred_indices:# or pred_idx not in exact_matched_pred_indices:\n",
    "                #if gold_drug == pred_drug:\n",
    "                    false_positives[relation_class].append({\n",
    "                        'text': text,\n",
    "                        'triple': (gold_sample[1:], pred_sample[1:]),\n",
    "                        'drug': pred_drug,\n",
    "                        'predicted_value': pred_value,\n",
    "                        'gold_values': gold_values\n",
    "                    })\n",
    "            \n",
    "        # Collect FN\n",
    "        for gold_idx, gold_value in enumerate(gold_values):\n",
    "            if gold_idx not in lenient_matched_gold_indices:# or gold_idx not in exact_matched_gold_indices:\n",
    "                if gold_drug == pred_drug:\n",
    "                    # Collect False Negatives\n",
    "                    false_negatives[relation_class].append({\n",
    "                        'text': text,\n",
    "                        'triple': (gold_sample[1:], pred_sample[1:]),\n",
    "                        'drug': gold_drug,\n",
    "                        'gold_value': gold_value,\n",
    "                        'predicted_values': pred_values\n",
    "                    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(metrics: Dict[str, Dict[str, int]], classes: Set[str], task: str, evaluation_type: str):\n",
    "    \"\"\"\n",
    "    Print classification reports.\n",
    "\n",
    "    Parameters:\n",
    "    - metrics dictionary containing metrics per class.\n",
    "    - classes set of classes.\n",
    "    - task: 'NER' or 'RE'\n",
    "    - evaluation_type: exact or lenient\n",
    "    \"\"\"\n",
    "    print(f\"\\n{task} {evaluation_type} Classification Reportn\")\n",
    "    header = f\"{'Class':<15} {'Support':<8} {'TP':<5} {'FP':<5} {'FN':<5} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    precision_sum = 0.0\n",
    "    recall_sum = 0.0\n",
    "    f1_sum = 0.0\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    for cls in sorted(classes):\n",
    "        if evaluation_type == 'Exact':\n",
    "            TP = metrics[cls]['Exact_TP']\n",
    "            FP = metrics[cls]['Exact_FP']\n",
    "            FN = metrics[cls]['Exact_FN']\n",
    "        elif evaluation_type == 'Lenient':\n",
    "            TP = metrics[cls]['Lenient_TP']\n",
    "            FP = metrics[cls]['Lenient_FP']\n",
    "            FN = metrics[cls]['Lenient_FN']\n",
    "        elif evaluation_type == 'Any':\n",
    "            TP = metrics[cls]['Any_TP']\n",
    "            FP = metrics[cls]['Any_FP']\n",
    "            FN = metrics[cls]['Any_FN']\n",
    "        else:\n",
    "            TP = FP = FN = 0\n",
    "        \n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        \n",
    "        precision_sum += precision\n",
    "        recall_sum += recall\n",
    "        f1_sum += f1\n",
    "        \n",
    "        print(f\"{cls:<15} {metrics[cls]['Support']:<8} {TP:<5} {FP:<5} {FN:<5} {precision:<10.3f} {recall:<10.3f} {f1:<10.3f}\")\n",
    "        \n",
    "    # compute micro average\n",
    "    if evaluation_type == 'Exact':\n",
    "        TP_micro = sum([metrics[cls]['Exact_TP'] for cls in classes])\n",
    "        FP_micro = sum([metrics[cls]['Exact_FP'] for cls in classes])\n",
    "        FN_micro = sum([metrics[cls]['Exact_FN'] for cls in classes])\n",
    "    elif evaluation_type == 'Lenient':\n",
    "        TP_micro = sum([metrics[cls]['Lenient_TP'] for cls in classes])\n",
    "        FP_micro = sum([metrics[cls]['Lenient_FP'] for cls in classes])\n",
    "        FN_micro = sum([metrics[cls]['Lenient_FN'] for cls in classes])\n",
    "    elif evaluation_type == 'Any':\n",
    "        TP_micro = sum([metrics[cls]['Any_TP'] for cls in classes])\n",
    "        FP_micro = sum([metrics[cls]['Any_FP'] for cls in classes])\n",
    "        FN_micro = sum([metrics[cls]['Any_FN'] for cls in classes])        \n",
    "    else:\n",
    "        TP_micro = FP_micro = FN_micro = 0\n",
    "    \n",
    "    precision_micro = TP_micro / (TP_micro + FP_micro) if (TP_micro + FP_micro) > 0 else 0.0\n",
    "    recall_micro = TP_micro / (TP_micro + FN_micro) if (TP_micro + FN_micro) > 0 else 0.0\n",
    "    f1_micro = 2 * precision_micro * recall_micro / (precision_micro + recall_micro) if (precision_micro + recall_micro) > 0 else 0.0\n",
    "    \n",
    "    # compute macro average\n",
    "    precision_macro = precision_sum / num_classes if num_classes > 0 else 0.0\n",
    "    recall_macro = recall_sum / num_classes if num_classes > 0 else 0.0\n",
    "    f1_macro = f1_sum / num_classes if num_classes > 0 else 0.0\n",
    "    \n",
    "    print(\"-\" * len(header))\n",
    "    print(f\"{'Micro Avg':<15} {'-':<5} {'-':<5} {'-':<5} {precision_micro:<10.3f} {recall_micro:<10.3f} {f1_micro:<10.3f}\")\n",
    "    print(f\"{'Macro Avg':<15} {'-':<5} {'-':<5} {'-':<5} {precision_macro:<10.3f} {recall_macro:<10.3f} {f1_macro:<10.3f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "def evaluate_model(gold_samples: List[OrderedDict], pred_samples: List[OrderedDict], complete: bool, use_medication_threshold = False, target_class = \"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model by computing and printing classification reports (NER, RE)\n",
    "\n",
    "    Parameters:\n",
    "    - gold_samples list of gold standard samples.\n",
    "    - pred_samples list of predicted samples.\n",
    "    \"\"\"\n",
    "    # extract relations for RE\n",
    "    gold_relations, pred_relations = extract_relations(gold_samples, pred_samples, use_medication_threshold)  \n",
    "    \n",
    "    # compute RE metrics\n",
    "    re_metrics = compute_re_metrics(gold_relations, pred_relations, classes=RE_CLASSES, complete=complete, target_class = target_class)\n",
    "    \n",
    "    # print Exact RE classification report\n",
    "    print_classification_report(re_metrics, RE_CLASSES, task='Relation Extraction (RE)', evaluation_type='Exact')\n",
    "    \n",
    "    # print Lenient RE classification report\n",
    "    print_classification_report(re_metrics, RE_CLASSES, task='Relation Extraction (RE)', evaluation_type='Lenient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Relation Extraction (RE) Exact Classification Reportn\n",
      "Class           Support  TP    FP    FN    Precision  Recall     F1-Score  \n",
      "---------------------------------------------------------------------------\n",
      "ade             3        2     0     1     1.000      0.667      0.800     \n",
      "dosage          0        0     0     0     0.000      0.000      0.000     \n",
      "duration        5        5     0     0     1.000      1.000      1.000     \n",
      "form            0        0     0     0     0.000      0.000      0.000     \n",
      "frequency       1        1     0     0     1.000      1.000      1.000     \n",
      "reason          13       12    4     1     0.750      0.923      0.828     \n",
      "route           1        1     0     0     1.000      1.000      1.000     \n",
      "strength        2        2     0     0     1.000      1.000      1.000     \n",
      "---------------------------------------------------------------------------\n",
      "Micro Avg       -     -     -     0.852      0.920      0.885     \n",
      "Macro Avg       -     -     -     0.719      0.699      0.703     \n",
      "\n",
      "\n",
      "\n",
      "Relation Extraction (RE) Lenient Classification Reportn\n",
      "Class           Support  TP    FP    FN    Precision  Recall     F1-Score  \n",
      "---------------------------------------------------------------------------\n",
      "ade             3        2     0     1     1.000      0.667      0.800     \n",
      "dosage          0        0     0     0     0.000      0.000      0.000     \n",
      "duration        5        5     0     0     1.000      1.000      1.000     \n",
      "form            0        0     0     0     0.000      0.000      0.000     \n",
      "frequency       1        1     0     0     1.000      1.000      1.000     \n",
      "reason          13       12    4     1     0.750      0.923      0.828     \n",
      "route           1        1     0     0     1.000      1.000      1.000     \n",
      "strength        2        2     0     0     1.000      1.000      1.000     \n",
      "---------------------------------------------------------------------------\n",
      "Micro Avg       -     -     -     0.852      0.920      0.885     \n",
      "Macro Avg       -     -     -     0.719      0.699      0.703     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define relation classes\n",
    "RE_CLASSES = {'ade', 'dosage', 'duration', 'form', 'frequency', 'reason', 'route', 'strength'}\n",
    "\n",
    "with open('output.csv', 'r', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter='|')\n",
    "    golds = []\n",
    "    preds = []\n",
    "    \n",
    "    for id, row in enumerate(reader):\n",
    "        text = row['text']\n",
    "\n",
    "        gold = eval(row['gold'], {\"OrderedDict\": OrderedDict}, {})\n",
    "        pred = eval(row['pred'], {\"OrderedDict\": OrderedDict}, {})\n",
    "\n",
    "        golds.append((text, gold))\n",
    "        preds.append((text, pred))\n",
    "        \n",
    "    evaluate_model(golds, preds, complete=True, use_medication_threshold=True, target_class=RE_CLASSES)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNSLOTH",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
