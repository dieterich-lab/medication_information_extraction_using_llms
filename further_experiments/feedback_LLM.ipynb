{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect false predictions for the feedback LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import dspy\n",
    "import ast\n",
    "import tqdm\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import os\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from typing import List, Tuple, Set, Dict, Optional, Union\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import difflib\n",
    "from collections import OrderedDict\n",
    "from functools import wraps\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "import json\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_counter(name):\n",
    "    \"\"\"\n",
    "    Extracts a counter (n) at the end of the medication.\n",
    "    Returns the medication name (without counter) and the counter.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\s*\\((\\d+)\\)\\s*$', name)\n",
    "    if match:\n",
    "        counter = match.group(1)\n",
    "        name_without_counter = name[:match.start()].strip()\n",
    "        return name_without_counter, counter\n",
    "    else:\n",
    "        return name.strip(), None\n",
    "\n",
    "def preprocess_med_name(name):\n",
    "    \"\"\"\n",
    "    Preprocess medication name:\n",
    "    - Extracting counter\n",
    "    - Lowercase\n",
    "    - Remove any content in parentheses (except the counter)\n",
    "    - Remove any punctuation except hyphens and spaces\n",
    "    - Remove spaces\n",
    "    \"\"\"\n",
    "    name_without_counter, counter = extract_counter(name)\n",
    "    name_clean = name_without_counter.lower()\n",
    "    name_clean = re.sub(r'\\([^)]*\\)', '', name_clean)\n",
    "    name_clean = re.sub(r'[^\\w\\s-]', '', name_clean)\n",
    "    name_clean = ' '.join(name_clean.split())\n",
    "    \n",
    "    return name_clean, counter\n",
    "\n",
    "def are_med_names_matching(name1, name2, threshold):\n",
    "    \"\"\"\n",
    "    Compare two medication names\n",
    "    \n",
    "    Parameters:\n",
    "    - med name 1, med name 2\n",
    "    - threshold: similarity score threshold\n",
    "    \n",
    "    Returns:\n",
    "    - True if match, False otherwise.\n",
    "    \"\"\"\n",
    "    name1_clean, counter1 = preprocess_med_name(name1)\n",
    "    name2_clean, counter2 = preprocess_med_name(name2)\n",
    "    \n",
    "    name1_clean = re.sub(r'\\b(\\w+)\\s+(n|en|e)\\b', r'\\1\\2', name1_clean)\n",
    "    name2_clean = re.sub(r'\\b(\\w+)\\s+(n|en|e)\\b', r'\\1\\2', name2_clean)\n",
    "    \n",
    "    # Check if the first characters are the same\n",
    "    if not name1_clean or not name2_clean:\n",
    "        return False, 0.0\n",
    "    if name1_clean[0] != name2_clean[0]:\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Similarity score of the medication names\n",
    "    ratio = difflib.SequenceMatcher(None, name1_clean, name2_clean).ratio()\n",
    "    \n",
    "    # If counters are different => not a match\n",
    "    if counter1 != counter2:\n",
    "        return False, ratio\n",
    "    \n",
    "    # If ratio >= threshold it is a match\n",
    "    return ratio >= threshold, ratio\n",
    "\n",
    "def adapt_medication_names(gold_dict, pred_dict, threshold):\n",
    "    \"\"\"\n",
    "    Compare medication names. If match align med name in pred and gold.\n",
    "    \n",
    "    Parameters:\n",
    "    - gold_dict: medication gold dict\n",
    "    - pred_dict: medication pred dict\n",
    "    - threshold: similarity score\n",
    "    \n",
    "    Returns:\n",
    "    - medication pred dict with aligned medication dict\n",
    "    \"\"\"\n",
    "    gold_meds = gold_dict.get('medications', [])\n",
    "    pred_meds = pred_dict.get('medications', [])\n",
    "    \n",
    "    # Check each medication in pred_dict\n",
    "    for pred_med in pred_meds:\n",
    "        pred_name = pred_med.get('medication', '')\n",
    "        best_match_name = None\n",
    "        highest_ratio = 0\n",
    "        \n",
    "        # Compare with each medication in gold_dict\n",
    "        for gold_med in gold_meds:\n",
    "            gold_name = gold_med.get('medication', '')\n",
    "            match, ratio = are_med_names_matching(pred_name, gold_name, threshold)\n",
    "            if match and ratio > highest_ratio:\n",
    "                highest_ratio = ratio\n",
    "                best_match_name = gold_name\n",
    "        \n",
    "        # If match is found, update the pred_med 'medication' name\n",
    "        if best_match_name:\n",
    "            pred_med['medication'] = best_match_name\n",
    "    \n",
    "    return pred_dict\n",
    "\n",
    "def reorder_dict(gold_samples, pred_samples):\n",
    "    \"\"\"\n",
    "    Order pred dict similar to gold_dict. in-place\n",
    "    \n",
    "    Parameters:\n",
    "    - gold_dict: medication gold dict\n",
    "    - pred_dict: medication pred dict\n",
    "    \"\"\"\n",
    "    for gold_sample, pred_sample in zip(gold_samples, pred_samples):\n",
    "        gold_dict = gold_sample[1]\n",
    "        pred_dict = pred_sample[1]\n",
    "\n",
    "        if len(gold_dict['medications']) > 0:\n",
    "            first_keys_order = list(gold_dict['medications'][0].keys())\n",
    "        \n",
    "            for i, med in enumerate(pred_dict['medications']):\n",
    "                sorted_med = OrderedDict((key, med[key]) for key in first_keys_order)            \n",
    "                pred_dict['medications'][i] = sorted_med\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "def ensure_keys_in_medications(data):\n",
    "    \"\"\"\n",
    "    Check, if all keys (relation information) is present. in-place.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: gold or pred dict\n",
    "    \"\"\"\n",
    "    RE_CLASSES_inclmed = RE_CLASSES.union({'medication'})\n",
    "    count = 0\n",
    "    for entry in data:\n",
    "        text, ordered_dict = entry\n",
    "        medications_list = ordered_dict.get('medications', [])\n",
    "        \n",
    "        for medication_dict in medications_list:\n",
    "            # Check for each required key, add if missing with an empty value\n",
    "            for key in RE_CLASSES_inclmed:\n",
    "                if key not in medication_dict:\n",
    "                    try:\n",
    "                        medication_dict[key] = ''  # Add missing key with an empty value\n",
    "                    except Exception as e:\n",
    "                        print(entry)\n",
    "                        count += 1\n",
    "                        print(f\"Error: {e} - {count}\")\n",
    "\n",
    "def extract_relations(gold_samples: List[Tuple[str, OrderedDict]], pred_samples: List[Tuple[str, OrderedDict]], use_medication_threshold: bool) -> Tuple[List[Tuple[str, str, str, str]], List[Tuple[str, str, str, str]]]:\n",
    "    \"\"\"\n",
    "    Extract relations from a list of samples. Ignore 'no drugs' sample\n",
    "\n",
    "    Parameters:\n",
    "    - gold_samples: List of gold samples\n",
    "    - pred_samples: List of pred samples\n",
    "\n",
    "    Returns:\n",
    "    - Tuples for gold and predicted samples.\n",
    "    \"\"\"\n",
    "        \n",
    "    gold_triples = []\n",
    "    pred_triples = []\n",
    "\n",
    "    ensure_keys_in_medications(gold_samples)\n",
    "    ensure_keys_in_medications(pred_samples)\n",
    "\n",
    "    reorder_dict(gold_samples, pred_samples)\n",
    "    \n",
    "    for gold, pred in zip(gold_samples, pred_samples):\n",
    "        gold_text, gold_dict = gold\n",
    "        pred_text, pred_dict = pred\n",
    "\n",
    "        # Ensure pred and gold texts are the same\n",
    "        assert gold_text == pred_text, \"Mismatch in texts between gold and pred.\"\n",
    "        gold_medications = gold_dict.get('medications', [])\n",
    "        pred_medications = pred_dict.get('medications', [])\n",
    "\n",
    "        if use_medication_threshold:\n",
    "            pred_dict = adapt_medication_names(gold_dict, pred_dict, threshold=0.75)\n",
    "\n",
    "        # Extract medication information from gold and pred\n",
    "        gold_medications = gold_dict.get('medications', [])\n",
    "        pred_medications = pred_dict.get('medications', [])\n",
    "        \n",
    "        # Convert list of medication dictionaries to lists with counts for comparison\n",
    "        gold_drugs_counts = defaultdict(int)  \n",
    "        pred_drugs_counts = defaultdict(int)  \n",
    "\n",
    "        for med in gold_medications:  \n",
    "            gold_drugs_counts[med['medication']] += 1  \n",
    "\n",
    "        for med in pred_medications:  \n",
    "            pred_drugs_counts[med['medication']] += 1\n",
    "\n",
    "        # Identify drugs with counts in both gold and predicted samples\n",
    "        common_drugs = set(gold_drugs_counts.keys()) & set(pred_drugs_counts.keys())\n",
    "        gold_only = set(gold_drugs_counts.keys()) - set(pred_drugs_counts.keys())\n",
    "        pred_only = set(pred_drugs_counts.keys()) - set(gold_drugs_counts.keys())\n",
    "\n",
    "        # Process drugs\n",
    "        for drug in common_drugs:\n",
    "            # Extract the correct number of instances for each drug\n",
    "            gold_props_list = [med for med in gold_medications if med['medication'] == drug]  \n",
    "            pred_props_list = [med for med in pred_medications if med['medication'] == drug]  \n",
    "\n",
    "            gold_instances_count = gold_drugs_counts[drug]  \n",
    "            pred_instances_count = pred_drugs_counts[drug]  \n",
    "            \n",
    "            common_count = min(gold_instances_count, pred_instances_count)  \n",
    "            gold_props_list = gold_props_list[:common_count]  \n",
    "            pred_props_list = pred_props_list[:common_count]\n",
    "\n",
    "            for gold_props in gold_props_list:\n",
    "                for rel_class, rel_value in gold_props.items():\n",
    "                    if rel_class != 'medication':\n",
    "                        gold_triples.append((gold_text, drug, rel_class, rel_value))\n",
    "\n",
    "            for pred_props in pred_props_list:\n",
    "                for rel_class, rel_value in pred_props.items():\n",
    "                    if rel_class != 'medication':\n",
    "                        pred_triples.append((pred_text, drug, rel_class, rel_value))\n",
    "        \n",
    "        # Process gold-only drugs\n",
    "        for drug in gold_only:\n",
    "            gold_props_list = [med for med in gold_medications if med['medication'] == drug]\n",
    "            for gold_props in gold_props_list:\n",
    "                for rel_class, rel_value in gold_props.items():\n",
    "                    if rel_class != 'medication':\n",
    "                        gold_triples.append((gold_text, drug, rel_class, rel_value))\n",
    "                        pred_triples.append((pred_text, 'no drugs', rel_class, ''))\n",
    "        \n",
    "        # Process pred-only drugs\n",
    "        for drug in pred_only:\n",
    "            pred_props_list = [med for med in pred_medications if med['medication'] == drug]\n",
    "            for pred_props in pred_props_list:\n",
    "                for rel_class, rel_value in pred_props.items():\n",
    "                    if rel_class != 'medication':\n",
    "                        pred_triples.append((pred_text, drug, rel_class, rel_value))\n",
    "                        gold_triples.append((gold_text, \"no drugs\", rel_class, ''))\n",
    "\n",
    "    return gold_triples, pred_triples\n",
    "\n",
    "def compute_re_metrics(gold_relations: List[Tuple[str, str, str, str]], pred_relations: List[Tuple[str, str, str, str]], classes: Set[str], complete: bool, target_class: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Collect exact and lenient TP, FP, FN for each relation class.\n",
    "\n",
    "    Parameters:\n",
    "    - gold_relations: list of gold (text, drug, class, value) tuples.\n",
    "    - pred_relations: list of pred (text, drug, class, value) tuples.\n",
    "    - classes: set of relation classes to evaluate.\n",
    "    - complete: whether to use the complete matching strategy. Meaning: if gold_drug != pred_drug => model failed to identify drug.\n",
    "    - target_class: target class to evaluate\n",
    "    Then all relation values are counted as FN and all pred_drug relation values are FP.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with class as key and metrics as value.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {cls: {\n",
    "        'Exact_TP': 0,\n",
    "        'Exact_FP': 0,\n",
    "        'Exact_FN': 0,\n",
    "        'Exact_TN': 0,\n",
    "        'Any_TP': 0,\n",
    "        'Any_FP': 0,\n",
    "        'Any_FN': 0,\n",
    "        'Any_TN': 0,\n",
    "        'Lenient_TP': 0,\n",
    "        'Lenient_FP': 0,\n",
    "        'Lenient_FN': 0,\n",
    "        'Support': 0 \n",
    "        } for cls in classes}\n",
    "\n",
    "    false_positives = {key: [] for key in classes}\n",
    "    false_negatives = {key: [] for key in classes}\n",
    "\n",
    "    # Normalize relation values\n",
    "    def normalize_relation_value(value, relation_class):\n",
    "        def normalize_word(word):\n",
    "            word = re.sub(r'\\.', '', word.strip().lower())            \n",
    "            word = re.sub(r'\\s*\\(s\\)\\s*', '', word)\n",
    "            return word.strip()\n",
    "    \n",
    "        # If input is string, normalize the string\n",
    "        if isinstance(value, str):\n",
    "            value = normalize_word(value)\n",
    "            return value\n",
    "    \n",
    "        # If input is list, normalize each element in the list\n",
    "        elif isinstance(value, list):\n",
    "            value = [normalize_word(v) for v in value]\n",
    "            return value\n",
    "    \n",
    "        # If the input is neither string nor list, return it as is\n",
    "        else:\n",
    "            return value\n",
    "        \n",
    "    def normalize_drug_name(drug_name):\n",
    "        # lowercase\n",
    "        drug_name = drug_name.lower()\n",
    "        # remove leading/trailing whitespace\n",
    "        drug_name = drug_name.strip()\n",
    "        # replace hyphens with a space\n",
    "        drug_name = drug_name.replace('-', ' ')\n",
    "        # remove special characters\n",
    "        drug_name = re.sub(r'[^\\w\\s]', '', drug_name)\n",
    "        # replace multiple spaces with single space\n",
    "        drug_name = re.sub(r'\\s+', ' ', drug_name)\n",
    "        # remove all whitespace and special characters\n",
    "        drug_name = re.sub(r'\\W+', '', drug_name)\n",
    "        return drug_name\n",
    "\n",
    "    for index in range(len(gold_relations)):\n",
    "        gold_sample = gold_relations[index]\n",
    "        pred_sample = pred_relations[index]\n",
    "        \n",
    "        text, gold_drug, gold_relation_class, gold_relation_value = gold_sample\n",
    "        text, pred_drug, pred_relation_class, pred_relation_value = pred_sample\n",
    "    \n",
    "        gold_relation_value = normalize_relation_value(gold_relation_value, gold_relation_class)\n",
    "        pred_relation_value = normalize_relation_value(pred_relation_value, gold_relation_class) \n",
    "\n",
    "        gold_drug = normalize_drug_name(gold_drug)\n",
    "        pred_drug = normalize_drug_name(pred_drug)       \n",
    "    \n",
    "        relation_class = gold_relation_class\n",
    "    \n",
    "        # Check if relation values are lists for consistent processing\n",
    "        gold_values = gold_relation_value if isinstance(gold_relation_value, list) else [gold_relation_value] if gold_relation_value else []\n",
    "        pred_values = pred_relation_value if isinstance(pred_relation_value, list) else [pred_relation_value] if pred_relation_value else []\n",
    "        \n",
    "        # DEBUGGING: Avoid typos for small 8b zero shot models\n",
    "        #if relation_class == 'dosagemg' or relation_class == 'dosagem':\n",
    "        #    relation_class = 'dosage'\n",
    "        #if relation_class == 'refills':\n",
    "        #    continue#break\n",
    "        #if relation_class == 'Ant':\n",
    "        #    continue#break\n",
    "            \n",
    "        # Update support value for the class\n",
    "        metrics[relation_class]['Support'] += len(gold_values)\n",
    "\n",
    "        # Initialize counts and matched indices for this sample\n",
    "        exact_tp = 0\n",
    "        lenient_tp = 0\n",
    "        exact_fp = 0\n",
    "        lenient_fp = 0\n",
    "        exact_fn = 0\n",
    "        lenient_fn = 0\n",
    "        \n",
    "        exact_matched_gold_indices = []\n",
    "        exact_matched_pred_indices = []\n",
    "        lenient_matched_gold_indices = []\n",
    "        lenient_matched_pred_indices = []\n",
    "\n",
    "        if gold_drug == pred_drug:\n",
    "            # Compare gold and predicted values for exact match\n",
    "            for gold_idx, gold_value in enumerate(gold_values):\n",
    "                exact_match_found = False\n",
    "                lenient_match_found = False\n",
    "                for pred_idx, pred_value in enumerate(pred_values):\n",
    "                    if gold_value == pred_value and pred_idx not in exact_matched_pred_indices:\n",
    "                        # Exact match{}\n",
    "                        exact_tp += 1\n",
    "                        exact_matched_gold_indices.append(gold_idx)\n",
    "                        exact_matched_pred_indices.append(pred_idx)\n",
    "                        # Also count exact as lenient match\n",
    "                        lenient_tp += 1\n",
    "                        lenient_matched_gold_indices.append(gold_idx)\n",
    "                        lenient_matched_pred_indices.append(pred_idx)\n",
    "                        exact_match_found = True\n",
    "                        break\n",
    "                # If no exact match, check for lenient match\n",
    "                if not exact_match_found:\n",
    "                    for pred_idx, pred_value in enumerate(pred_values):\n",
    "                        if pred_idx not in lenient_matched_pred_indices:\n",
    "                            gold_words = gold_value.split()\n",
    "                            pred_words = pred_value.split()\n",
    "                            gold_words = [word.strip(string.punctuation).lower() for word in gold_words]\n",
    "                            pred_words = [word.strip(string.punctuation).lower() for word in pred_words]\n",
    "                            if any(word in pred_words for word in gold_words):\n",
    "                                lenient_tp += 1\n",
    "                                lenient_matched_gold_indices.append(gold_idx)\n",
    "                                lenient_matched_pred_indices.append(pred_idx)\n",
    "                                lenient_match_found = True\n",
    "                                break\n",
    "                    # Count exact FN if no exact match was found, regardless of lenient match\n",
    "                    exact_fn += 1\n",
    "                    if not lenient_match_found:\n",
    "                        # Increment Lenient_FN if no lenient match was found\n",
    "                        lenient_fn += 1\n",
    "        \n",
    "            # Any unmatched predicted values are false positives\n",
    "            for pred_idx, pred_value in enumerate(pred_values):\n",
    "                if pred_idx not in exact_matched_pred_indices:\n",
    "                    exact_fp += 1\n",
    "                if pred_idx not in lenient_matched_pred_indices:\n",
    "                    lenient_fp += 1\n",
    "        \n",
    "        else:\n",
    "            if complete:\n",
    "                # Drug names do not match; count all gold values as FN and predicted values as FP\n",
    "                exact_fn += len(gold_values)\n",
    "                lenient_fn += len(gold_values)\n",
    "                exact_fp += len(pred_values)\n",
    "                lenient_fp += len(pred_values)\n",
    "            else:\n",
    "                # Drug names do not match; count only per missed drug name as FN and predicted values as FP             \n",
    "                exact_fn += 1 if len(gold_values) >= 1 else 0 #len(gold_values)\n",
    "                lenient_fn += 1 if len(gold_values) >= 1 else 0 #len(gold_values)\n",
    "                exact_fp += 1 if len(pred_values) >= 1 else 0 #len(pred_values)\n",
    "                lenient_fp += 1 if len(pred_values) >= 1 else 0 #len(pred_values)                \n",
    "    \n",
    "        # Update metrics\n",
    "        metrics[relation_class]['Exact_TP'] += exact_tp\n",
    "        metrics[relation_class]['Exact_FP'] += exact_fp\n",
    "        metrics[relation_class]['Exact_FN'] += exact_fn\n",
    "        \n",
    "        metrics[relation_class]['Lenient_TP'] += lenient_tp\n",
    "        metrics[relation_class]['Lenient_FP'] += lenient_fp\n",
    "        metrics[relation_class]['Lenient_FN'] += lenient_fn        \n",
    "\n",
    "        # Collect FP\n",
    "        for pred_idx, pred_value in enumerate(pred_values):\n",
    "            if pred_idx not in lenient_matched_pred_indices:# or pred_idx not in exact_matched_pred_indices:\n",
    "                #if gold_drug == pred_drug:\n",
    "                    false_positives[relation_class].append({\n",
    "                        'text': text,\n",
    "                        'triple': (gold_sample[1:], pred_sample[1:]),\n",
    "                        'drug': pred_drug,\n",
    "                        'predicted_value': pred_value,\n",
    "                        'gold_values': gold_values\n",
    "                    })\n",
    "            \n",
    "        # Collect FN\n",
    "        for gold_idx, gold_value in enumerate(gold_values):\n",
    "            if gold_idx not in lenient_matched_gold_indices:# or gold_idx not in exact_matched_gold_indices:\n",
    "                if gold_drug == pred_drug:\n",
    "                    # Collect False Negatives\n",
    "                    false_negatives[relation_class].append({\n",
    "                        'text': text,\n",
    "                        'triple': (gold_sample[1:], pred_sample[1:]),\n",
    "                        'drug': gold_drug,\n",
    "                        'gold_value': gold_value,\n",
    "                        'predicted_values': pred_values\n",
    "                    })\n",
    "\n",
    "    # Write false negatives and positives into a file\n",
    "    os.makedirs(os.path.dirname(\"fp_fn/i2b2/\"), exist_ok=True)\n",
    "\n",
    "    with open(f\"fp_fn/i2b2/{target_class}_fp.txt\", \"w\") as f_p:\n",
    "        writer = csv.writer(f_p, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "        for fp in false_positives[target_class]:\n",
    "            if len(fp['gold_values']) > 0:\n",
    "                writer.writerow([fp['text'], str(fp['triple'])])\n",
    "\n",
    "    \n",
    "    with open(f\"fp_fn/i2b2/{target_class}_fn.txt\", \"w\") as f_n:\n",
    "        writer = csv.writer(f_n, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "        for fn in false_negatives[target_class]:\n",
    "            if len(fn['predicted_values']) > 0:\n",
    "                writer.writerow([fn['text'], str(fn['triple'])])\n",
    "       \n",
    "\n",
    "    return metrics\n",
    "\n",
    "def evaluate_model(gold_samples: List[OrderedDict], pred_samples: List[OrderedDict], complete: bool, use_medication_threshold = False, target_class = \"\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model by computing and printing classification reports for NER and RE.\n",
    "\n",
    "    Args:\n",
    "        gold_samples (List[OrderedDict]): List of gold standard samples.\n",
    "        pred_samples (List[OrderedDict]): List of predicted samples.\n",
    "    \"\"\"\n",
    "    # Extract relations for RE\n",
    "    gold_relations, pred_relations = extract_relations(gold_samples, pred_samples, use_medication_threshold)  \n",
    "    \n",
    "    \n",
    "    # Compute RE metrics\n",
    "    re_metrics = compute_re_metrics(gold_relations, pred_relations, classes=RE_CLASSES, complete=complete, target_class = target_class)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract false predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relation classes\n",
    "RE_CLASSES = {'ade', 'dosage', 'duration', 'form', 'frequency', 'reason', 'route', 'strength'}\n",
    "\n",
    "for target_class in RE_CLASSES:\n",
    "    with open('output_results_llama3_8b_i2b2_pydantic.csv', 'r', newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter='|')\n",
    "        golds = []\n",
    "        preds = []\n",
    "        \n",
    "        for id, row in enumerate(reader):\n",
    "            text = row['text']\n",
    "\n",
    "            gold = eval(row['gold'], {\"OrderedDict\": OrderedDict}, {})\n",
    "            pred = eval(row['pred'], {\"OrderedDict\": OrderedDict}, {})\n",
    "\n",
    "            golds.append((text, gold))\n",
    "            preds.append((text, pred))\n",
    "            \n",
    "        evaluate_model(golds[:], preds[:], complete=True, use_medication_threshold=True, target_class=target_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply feedback LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and define prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "chatbot = pipeline(\"text-generation\", model=\"unsloth/Mistral-Large-Instruct-2407-bnb-4bit\")\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a doctor specializing in pharmacology. You receive a text along with two triplets containing medication information: the gold standard (Gold) and the model prediction (Pred), which may include false positives and false negatives. Each triplet includes:\n",
    "- the medication name,\n",
    "- the category of medication information,\n",
    "- and a value.\n",
    "\n",
    "The context of the text should be taken into account to ensure the clinical meaning of the values is fully understood and no subtleties are overlooked.\n",
    "\n",
    "Your task is to evaluate carefully and conservatively whether the values in the two triplets are clinically comparable. A classification of *SIMILAR* should only be made if the similarity is obvious and clearly clinically meaningful!\n",
    "\n",
    "Please proceed with the evaluation as follows:\n",
    "1. Carefully read the text to understand the context of the medication information.\n",
    "2. Compare the values in the Gold and Pred triplets.\n",
    "3. Determine if the values are clinically equivalent or comparable in the given context.\n",
    "4. If the values are clearly similar within the context of the text, provide the result as *Result: SIMILAR*.\n",
    "5. If the values are not clearly comparable or have a divergent clinical meaning, provide the result as *Result: NOT SIMILAR*.\n",
    "\n",
    "**Example of structure:**\n",
    "\n",
    "Text: The patient administers insulin a day to manage blood sugar levels\n",
    "Triplets: (('insulin', 'frequency', ''), ('insulin', 'frequency', 'day')): The model predicts a false positive. Day is not in the gold standard. Result: NOT SIMILAR\n",
    "\n",
    "Text: he patientâ€™s methotrexate regimen includes doses on qFri and qSat, administered weekly to ensure consistent therapeutic levels.\n",
    "Triplets: (('methotrexate', 'frequency', 'qFri, qSat'), ('methotrexate', 'frequency', ['qFri', 'qSat'])): Both values are similar. The mdodel splits the information qFri and qSat into two list items, while the gold standard contains these information in a single string. Result: SIMILAR\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply feedback LLM per relation class and store answers in feedback files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect FPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fp=[]\n",
    "for target_class in RE_CLASSES:\n",
    "    print(target_class)\n",
    "    fp_file_path = f\"fp_fn/i2b2/{target_class}_fp.txt\"\n",
    "    \n",
    "    # Count the total lines in the file to use with tqdmp\n",
    "    with open(fp_file_path, \"r\") as f_p:\n",
    "        total_lines = sum(1 for _ in f_p)\n",
    "\n",
    "    with open(f\"fp_fn/i2b2/{target_class}_fp_feedback.txt\", \"w\") as f_p_feedback:\n",
    "        with open(fp_file_path, \"r\") as f_p:\n",
    "            reader = csv.reader(f_p, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "            for row in tqdm.tqdm(reader, total=total_lines):\n",
    "                text, triple = row[0], row[1]\n",
    "                user_content = f\"Text: {text}, Triplets: {triple}\"\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": user_content},\n",
    "                ]\n",
    "                answer = chatbot(messages)[-1]\n",
    "                response_text = answer['generated_text'][-1]['content']\n",
    "\n",
    "                output_fp.append(response_text)\n",
    "                cleaned_response_text = response_text.replace(\"\\n\", \" \")\n",
    "                f_p_feedback.write(f\"{user_content} ---- {cleaned_response_text}\\n\")\n",
    "                f_p_feedback.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect FNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fn=[]\n",
    "for target_class in RE_CLASSES:\n",
    "    print(target_class)\n",
    "    fn_file_path = f\"fp_fn/cardiode/{target_class}_fn.txt\"\n",
    "    \n",
    "    # Count the total lines in the file to use with tqdmp\n",
    "    with open(fn_file_path, \"r\") as f_n:\n",
    "        total_lines = sum(1 for _ in f_n)\n",
    "\n",
    "    with open(f\"fp_fn/cardiode/{target_class}_fn_feedback.txt\", \"w\") as f_n_feedback:\n",
    "        with open(fn_file_path, \"r\") as f_n:\n",
    "            reader = csv.reader(f_n, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "            for row in tqdm.tqdm(reader, total=total_lines):\n",
    "                text, triple = row[0], row[1]\n",
    "                user_content = f\"Text: {text}, Triplets: {triple}\"\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": user_content},\n",
    "                ]\n",
    "                answer = chatbot(messages)[-1]\n",
    "                response_text = answer['generated_text'][-1]['content']\n",
    "\n",
    "                output_fn.append(response_text)\n",
    "                f_n_feedback.write(f\"{user_content} ---- {response_text.replace('\\n', ' ')}\\n\")\n",
    "                f_n_feedback.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
