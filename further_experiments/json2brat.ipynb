{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert JSON output to BRAT \n",
    "\n",
    "This is an experimental version to convert the custom JSON data back to BRAT format\n",
    "\n",
    "It is based on heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import ast\n",
    "import json\n",
    "import pathlib\n",
    "import re\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert JSON to BRAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global vars\n",
    "\n",
    "BASE_DATA_DIR = pathlib.Path(\"/prj/doctoral_letters/data/i2b2_2018track2/test\")\n",
    "CSV_PATH      = pathlib.Path(\n",
    "    \"/home/prichter/research/MIEQA/output_results_llama3_8b_i2b2_pydantic.csv\"\n",
    ")\n",
    "OUT_DIR       = pathlib.Path(\"json2brat\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "ANN_DIR    = pathlib.Path(\"json2brat/system\")          # converted .ann files\n",
    "ANN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "ENTITY_KEYS = [\"strength\",\"dosage\",\"form\",\"frequency\",\"route\",\"reason\",\"ade\",\"duration\"]\n",
    "\n",
    "# HELPERS\n",
    "\n",
    "_ADM_RE = re.compile(\n",
    "    r\"Admission\\s+Date:\\s*\\[\\*\\*([0-9]{4}-[0-9]{1,2}-[0-9]{1,2})\\*\\*\\]\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "def _build_letter_map(csv_path: pathlib.Path) -> Dict[str, List[dict]]:\n",
    "    \"\"\"Return {'[**2122-1-14**]': [med_dict, ...], ...} aggregated over the CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", names=[\"text\", \"gold\", \"pred\"], header=0)\n",
    "\n",
    "    letter_map: Dict[str, List[dict]] = defaultdict(list)\n",
    "    current_token = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text_line: str = str(row[\"text\"])\n",
    "        m = _ADM_RE.search(text_line)\n",
    "        if m:\n",
    "            current_token = f\"[**{m.group(1)}**]\"\n",
    "\n",
    "        if current_token is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pred_od = eval(row[\"pred\"])\n",
    "            meds = pred_od.get(\"medications\", [])\n",
    "            letter_map[current_token].extend(meds)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return letter_map\n",
    "\n",
    "LETTER_PRED_MAP = _build_letter_map(CSV_PATH)\n",
    "\n",
    "\n",
    "def load_note(letter_id: str) -> str:\n",
    "    return (BASE_DATA_DIR / f\"{letter_id}.txt\").read_text(encoding=\"utf-8\")\n",
    "\n",
    "def extract_admission_token(note_text: str) -> str:\n",
    "    m = _ADM_RE.search(note_text)\n",
    "    if not m:\n",
    "        raise ValueError(\"Admission Date token not found in original note\")\n",
    "    return f\"[**{m.group(1)}**]\"\n",
    "\n",
    "def normalise_spaces(s: str) -> str:\n",
    "    import re\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip())\n",
    "\n",
    "def find_offset(text: str, substring: str, used_spans: Set[Tuple[int, int]],\n",
    "                start_pos: int = 0) -> Tuple[int, int]:\n",
    "    if not substring:\n",
    "        raise ValueError(\"Empty substring\")\n",
    "    while True:\n",
    "        idx = text.find(substring, start_pos)\n",
    "        if idx == -1:\n",
    "            break\n",
    "        span = (idx, idx + len(substring))\n",
    "        if all(not (max(a, span[0]) < min(b, span[1])) for a, b in used_spans):\n",
    "            return span\n",
    "        start_pos = idx + 1\n",
    "    raise ValueError(f\"Could not locate unique span for: {substring!r}\")\n",
    "\n",
    "ENTITY_MAP = {\n",
    "    \"medication\": (\"Drug\", \"Drug\"),\n",
    "    \"strength\":   (\"Strength\", \"Strength-Drug\"),\n",
    "    \"dosage\":     (\"Dosage\", \"Dosage-Drug\"),\n",
    "    \"form\":       (\"Form\", \"Form-Drug\"),\n",
    "    \"frequency\":  (\"Frequency\", \"Frequency-Drug\"),\n",
    "    \"route\":      (\"Route\", \"Route-Drug\"),\n",
    "    \"reason\":     (\"Reason\", \"Reason-Drug\"),\n",
    "    \"ade\":        (\"ADE\", \"ADE-Drug\"),\n",
    "    \"duration\":   (\"Duration\", \"Duration-Drug\"),\n",
    "}\n",
    "\n",
    "def convert_one_letter(letter_id: str) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Produce .ann files\n",
    "    \"\"\"\n",
    "    note_text = load_note(letter_id)\n",
    "    adm_token = extract_admission_token(note_text)\n",
    "\n",
    "    meds = LETTER_PRED_MAP.get(adm_token, [])\n",
    "    if not meds:\n",
    "        raise ValueError(f\"No prediction rows found for admission token {adm_token}\")\n",
    "\n",
    "    ann_lines: List[str] = []\n",
    "    used: Set[Tuple[int, int]] = set()\n",
    "    tid = rid = 1\n",
    "\n",
    "    def next_tid():\n",
    "        nonlocal tid\n",
    "        tid += 1\n",
    "        return f\"T{tid-1}\"\n",
    "\n",
    "    def next_rid():\n",
    "        nonlocal rid\n",
    "        rid += 1\n",
    "        return f\"R{rid-1}\"\n",
    "\n",
    "    for med in meds:\n",
    "        drug_name_raw = normalise_spaces(med.get(\"medication\", \"\"))\n",
    "        drug_name = re.sub(r\"\\s*\\(\\d+\\)\\s*$\", \"\", drug_name_raw)\n",
    "        try:\n",
    "            s, e = find_offset(note_text, drug_name, used)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        drug_tid = next_tid()\n",
    "        ann_lines.append(f\"{drug_tid}\\tDrug {s} {e}\\t{drug_name}\")\n",
    "        used.add((s, e))\n",
    "\n",
    "        for key, val in med.items():\n",
    "            if key == \"medication\" or not val:\n",
    "                continue\n",
    "            vals = val if isinstance(val, list) else [val]\n",
    "            for v in vals:\n",
    "                v_norm = normalise_spaces(v)\n",
    "                try:\n",
    "                    s2, e2 = find_offset(note_text, v_norm, used)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                ent_type, rel_type = ENTITY_MAP[key]\n",
    "                ent_tid = next_tid()\n",
    "                ann_lines.append(f\"{ent_tid}\\t{ent_type} {s2} {e2}\\t{v_norm}\")\n",
    "                used.add((s2, e2))\n",
    "                ann_lines.append(f\"{next_rid()}\\t{rel_type} Arg1:{ent_tid} Arg2:{drug_tid}\")\n",
    "\n",
    "    out_path = OUT_DIR / f\"system/{letter_id}.ann\"\n",
    "    out_path.write_text(\"\\n\".join(ann_lines), encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "outs = []\n",
    "for txt_path in BASE_DATA_DIR.glob(\"*.txt\"):\n",
    "    letter_id = txt_path.stem          # e.g. \"107515\"\n",
    "    outs.append(convert_one_letter(letter_id))\n",
    "\n",
    "print(f\"Wrote {len(outs)} .ann files to {OUT_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers ----------------------------------------------------------\n",
    "def parse_ann_to_counters(path):\n",
    "    \"\"\"\n",
    "    Return two Counters:\n",
    "    - ent_counter\n",
    "    - rel_counter\n",
    "    \"\"\"\n",
    "    ent_counter = Counter()\n",
    "    rel_counter = Counter()\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            if line.startswith(\"T\"):\n",
    "                parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                if len(parts) != 3:\n",
    "                    continue         \n",
    "                label = parts[1].split()[0]\n",
    "                text  = parts[2]\n",
    "                ent_counter[(label, text)] += 1\n",
    "\n",
    "            elif line.startswith(\"R\"):\n",
    "                parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "                if len(parts) != 2:\n",
    "                    continue       \n",
    "                rel_type = parts[1].split()[0]\n",
    "                rel_counter[(rel_type, \"\", \"\")] += 1\n",
    "\n",
    "    return ent_counter, rel_counter\n",
    "\n",
    "# gold counts from CSV \n",
    "gold_ent      = Counter()      \n",
    "gold_rel      = Counter()       \n",
    "gold_ent_cls  = Counter()\n",
    "gold_rel_cls  = Counter()\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, sep=\"|\", names=[\"text\",\"gold\",\"pred\"], header=1)\n",
    "for od in (eval(g) for g in df[\"gold\"]):\n",
    "    for m in od.get(\"medications\", []):\n",
    "        drug = m.get(\"medication\",\"\").strip()\n",
    "        if drug:\n",
    "            gold_ent[(\"Drug\", drug)] += 1\n",
    "            gold_ent_cls[\"Drug\"] += 1\n",
    "        for k in ENTITY_KEYS:\n",
    "            val = m.get(k,'')\n",
    "            if not val: continue\n",
    "            for v in (val if isinstance(val, list) else [val]):\n",
    "                v = v.strip()\n",
    "                if not v: continue\n",
    "                lbl = k.upper() if k == \"ade\" else k.capitalize()\n",
    "\n",
    "                gold_ent[(lbl, v)] += 1 ; gold_ent_cls[lbl] += 1\n",
    "                rel_lbl = f\"{lbl}-Drug\"\n",
    "                gold_rel[(rel_lbl, v, drug)] += 1\n",
    "                gold_rel_cls[rel_lbl] += 1\n",
    "\n",
    "tmp = defaultdict(int)\n",
    "for (lbl, _, _), cnt in gold_rel.items():\n",
    "    tmp[(lbl, '', '')] += cnt\n",
    "gold_rel = Counter(tmp)\n",
    "\n",
    "# parse all converted .ann files \n",
    "pred_ent = Counter(); pred_rel = Counter()\n",
    "for ann in ANN_DIR.glob(\"*.ann\"):\n",
    "    e_cnt, r_cnt = parse_ann_to_counters(ann)\n",
    "    pred_ent += e_cnt ; pred_rel += r_cnt\n",
    "\n",
    "# corpus-level numbers \n",
    "tot_gold_e = sum(gold_ent.values());   tot_pred_e = sum(pred_ent.values())\n",
    "tot_gold_r = sum(gold_rel.values());   tot_pred_r = sum(pred_rel.values())\n",
    "\n",
    "# diff counters\n",
    "missing_ent_keys = {}      \n",
    "add_ent_keys     = {}      \n",
    "missing_rel_keys = {}     \n",
    "add_rel_keys     = {}     \n",
    "\n",
    "# entities\n",
    "for k, gold_n in gold_ent.items():\n",
    "    diff = gold_n - pred_ent.get(k, 0)\n",
    "    if diff > 0:                 # missing\n",
    "        missing_ent_keys[k] = diff\n",
    "\n",
    "for k, pred_n in pred_ent.items():\n",
    "    diff = pred_n - gold_ent.get(k, 0)\n",
    "    if diff > 0:                 # additional\n",
    "        add_ent_keys[k] = diff\n",
    "\n",
    "# relations  (coarse keys = (relType,'',''))\n",
    "counter = 1\n",
    "for k, gold_n in gold_rel.items():\n",
    "    diff = gold_n - pred_rel.get(k, 0)\n",
    "    if diff > 0:\n",
    "        missing_rel_keys[k] = diff\n",
    "\n",
    "for k, pred_n in pred_rel.items():\n",
    "    diff = pred_n - gold_rel.get(k, 0)\n",
    "    if diff > 0:\n",
    "        add_rel_keys[k] = diff\n",
    "\n",
    "# corpus-level numbers\n",
    "missing_e = sum(missing_ent_keys.values())\n",
    "add_e     = sum(add_ent_keys.values())\n",
    "missing_r = sum(missing_rel_keys.values())\n",
    "add_r     = sum(add_rel_keys.values())\n",
    "\n",
    "print(\"\\n=== CSV-gold  vs  converted-BRAT (duplicate-aware) ===\")\n",
    "print(f\"Entities gold : {tot_gold_e}\")\n",
    "print(f\"Entities ann  : {tot_pred_e}\")\n",
    "print(f\"  missing     : {missing_e}\")\n",
    "print(f\"  additional  : {add_e}\\n\")\n",
    "print(f\"Relations gold: {tot_gold_r}\")\n",
    "print(f\"Relations ann : {tot_pred_r}\")\n",
    "print(f\"  missing     : {missing_r}\")\n",
    "print(f\"  additional  : {add_r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare gold-JSON vs converted-BRAT relation counts per class\n",
    "\n",
    "pred_rel_cls = Counter()\n",
    "for (rel_type, _, _), cnt in pred_rel.items():\n",
    "    pred_rel_cls[rel_type] += cnt\n",
    "\n",
    "print(\"--- Relation counts: CSV-gold  vs  BRAT ---\")\n",
    "print(f\"{'Relation':<18}{'gold':>8}{'ann':>8}{'Δ':>8}{'%Δ':>8}\")\n",
    "print(\"-\"*42)\n",
    "for rel in sorted(gold_rel_cls):\n",
    "    gold_n = gold_rel_cls[rel]\n",
    "    pred_n = pred_rel_cls.get(rel, 0)\n",
    "    delta  = pred_n - gold_n\n",
    "    pct    = (delta / gold_n * 100) if gold_n else 0\n",
    "    print(f\"{rel:<18}{gold_n:>8}{pred_n:>8}{delta:>8}{pct:>7.1f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
