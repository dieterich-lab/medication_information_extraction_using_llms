{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List, Union\n",
    "from pydantic import Field\n",
    "import json\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import dspy\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import ast\n",
    "import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "from json_repair import repair_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined = \"Meta-Llama-3_1-8b_refined_i2b2_pydantic/\"\n",
    "\n",
    "max_seq_length = 2048\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = refined,\n",
    "    max_seq_length = max_seq_length,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset from a file\n",
    "with open('./data/n2c2_test.pkl', 'rb') as f:\n",
    "    dataset_test = pickle.load(f)\n",
    "\n",
    "# Convert to a list of dictionaries\n",
    "test_dict_data = [{'prompt': item.paragraph, 'response': item.answer} for item in dataset_test]\n",
    "\n",
    "# Create a Dataset\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame(test_dict_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to CHATML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Medication(BaseModel):\n",
    "    \"\"\"Medication information extracted from the text.\"\"\"\n",
    "    medication: str = Field(description=\"A drug name or an active ingredient.\")\n",
    "    ade: Optional[Union[str, List[str]]] = Field(default=\"\", description=\"Extract adverse drug events from the text. Example: rash, hypotension, thrombocytopenia, toxicity, diarrhea, altered mental status, Rash, Thrombocytopenia, GI bleed, somnolent, etc.\")\n",
    "    strength: Optional[Union[str, List[str]]] = Field(default=\"\", description=\"Extract the strength of the medication from the text. Examples: 100 mg, 10 mg, 5 mg, 20 mg, 40 mg, 25 mg, 500 mg, 10mg, 50 mg, 5mg, etc.\")\n",
    "    frequency: Optional[Union[str, List[str]]] = Field(default=\"\", description=\"Extract the frequency of the medication from the text. Examples: 1-0-0, 1-0-1, daily, 0-0-1, DAILY (Daily), once a day, DAILY, BID, BID (2 times a day), twice a day, etc.\")\n",
    "    duration: Optional[Union[str, List[str]]] = Field(default=\"\", description=\"Extract the duration of the medication from the text. Examples: dauerhaft, pausiert, abgesetzt, f√ºr 12 Monate, B-DATE - B-DATE, Pause, dauerhafte, 14 day, for 7 days, for 10 days, etc.\")\n",
    "    route: Optional[Union[str, List[str]]] = Field(default=\"\", description=\"Extract the route of the medication from the text. Examples: PO, Oral, IV, by mouth, po, Inhalation, oral, drip, gtt, i.v., etc.\")\n",
    "    form: Optional[Union[str, List[str]]] = Field(default=\"\", description=\"Extract the form of the medication from the text. Examples: Tablet, Capsule, Solution, Tablet, Delayed Release (E.C.), Tablets, Tablet, Chewable, tablet, Appl, Capsule, Delayed Release(E.C.), Tablet(s), etc.\")\n",
    "    dosage: Optional[Union[str, List[str]]] = Field(default=\"\", description=\"Extract the dosage of the medication from the text. Examples: One (1), Two (2), 1, 1-2, 2, sliding scale, Three (3), 0.5, taper, 3, etc.\")\n",
    "    reason: Optional[Union[str, List[str]]] = Field(default=\"\", description=\"Extract the reason of the medication from the text. Examples: pain, Antikoagulation, constipation, Thrombozytenaggregationshemmung, Stentverschlussprophylaxe, anxiety, pneumonia, Antibiose, duale Thrombozytenaggregationshemmung, wheezing, etc.\")\n",
    "\n",
    "class MedicationInfo(BaseModel):\n",
    "    \"\"\"A list of medication information extracted from the text.\"\"\"\n",
    "    medications: List[Medication] = Field(default_factory=list, description=\"A list of medications and their related information.\")\n",
    "\n",
    "schema = json.dumps(MedicationInfo.model_json_schema())\n",
    "\n",
    "# Convert dataset to conversational format\n",
    "system_message = f\"\"\"You are a physician. Your task is to extract ALL drug names (active ingredients or drug names) and their related information, such as ADE, strength, frequency, duration, route, form, dosage, and reason from a given text snippet of a doctoral letter. \n",
    "Please make sure to extract the medications **in the order they appear** in the text. Maintain this order in the JSON response.\n",
    "If a medication occurs more than once in the text, append a unique count in parentheses to its name, starting from (1). \n",
    "If there is NO medication information in the text, create a this JSON: {{'medications': []}}\n",
    "ONLY respond with an instance of JSON without any additional information. You have access to a JSON schema, which will determine how the JSON should be structured.\n",
    "Make sure to return ONLY an instance of the JSON, NOT the schema itself. Do not add any additional information.\n",
    "JSON schema:\n",
    "{schema}\n",
    "\"\"\"\n",
    "\n",
    "# Define conversation format\n",
    "def create_conversation(sample):\n",
    "    conversation = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message},  \n",
    "            {\"role\": \"user\", \"content\": sample[\"prompt\"]},  \n",
    "            {\"role\": \"assistant\", \"content\": sample[\"response\"]}  \n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return conversation\n",
    "\n",
    "# Apply the conversation function to the dataset\n",
    "test_chat_dataset = test_dataset.map(create_conversation, remove_columns=['prompt', 'response'])\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    ")\n",
    "\n",
    "# Apply the CHATML format\n",
    "def apply_template(examples):\n",
    "    messages = examples[\"messages\"]\n",
    "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
    "    return {\"text\": text}\n",
    "\n",
    "final_test = test_chat_dataset.map(apply_template, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Initialize the count of malformed JSON predictions\n",
    "malformed_json_preds = 0\n",
    "\n",
    "pattern = r\"(?<=\\w)'(?=\\w)\"\n",
    "\n",
    "# Process all messages\n",
    "all_messages = test_chat_dataset['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to '|' delimited output csv\n",
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter='|')\n",
    "            csvwriter.writerow(['text', 'gold', 'pred'])\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                for batch_start in tqdm.tqdm(range(0, len(all_messages), batch_size)):\n",
    "                    batch = all_messages[batch_start:batch_start + batch_size]\n",
    "        \n",
    "                    texts = []\n",
    "                    golds = []\n",
    "                    messages = []\n",
    "        \n",
    "                    for i in batch:\n",
    "                        text = i[1]['content'].strip().replace(\"\\n\", \"\")\n",
    "                        texts.append(text)\n",
    "        \n",
    "                        gold = i[2]['content']\n",
    "                        try:\n",
    "                            gold_cleaned = re.sub(pattern, \"\", gold)\n",
    "                            gold_dict = ast.literal_eval(gold_cleaned)\n",
    "                            sorted_gold_dict = OrderedDict(sorted(gold_dict.items()))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error parsing gold data: {e}\")\n",
    "                            sorted_gold_dict = {}\n",
    "                        golds.append(sorted_gold_dict)\n",
    "        \n",
    "                        messages.append(i[:2])\n",
    "        \n",
    "                    if not messages:\n",
    "                        continue\n",
    "        \n",
    "                    # Convert the input texts using the CHATML template\n",
    "                    input_texts = [tokenizer.apply_chat_template(\n",
    "                        message,\n",
    "                        tokenize=False,\n",
    "                        add_generation_prompt=True\n",
    "                    ) for message in messages]\n",
    "        \n",
    "                    # Tokenize the rendered texts\n",
    "                    inputs = tokenizer(\n",
    "                        input_texts,\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=max_seq_length,\n",
    "                        return_tensors='pt'\n",
    "                    ).to(device)\n",
    "        \n",
    "                    # Generate deterministic outputs\n",
    "                    outputs = model.generate(\n",
    "                        input_ids=inputs['input_ids'],\n",
    "                        attention_mask=inputs.get('attention_mask'),\n",
    "                        max_new_tokens=384,\n",
    "                        use_cache=True,\n",
    "                        top_p=1.0,\n",
    "                        temperature=1.0,\n",
    "                        do_sample=False\n",
    "                    )\n",
    "        \n",
    "                    # Decode output text\n",
    "                    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "                    # Process each sample in the batch and write to csv\n",
    "                    for idx in range(len(decoded_outputs)):\n",
    "                        try:\n",
    "                            decoded_output = decoded_outputs[idx]\n",
    "                            assistant_response = decoded_output.split(\"<|im_start|>assistant\")[1].replace(\"<|im_end|>\", \"\").strip()\n",
    "        \n",
    "                            # Convert the assistant's response to a sorted dictionary\n",
    "                            pred_cleaned = re.sub(pattern, \"\", assistant_response)\n",
    "                            pred_cleaned = repair_json(assistant_response)\n",
    "                            pred_dict = ast.literal_eval(pred_cleaned)\n",
    "                            sorted_pred_dict = OrderedDict(sorted(pred_dict.items()))\n",
    "        \n",
    "                        except Exception as e_sample:\n",
    "                            # Document malformed JSON output\n",
    "                            print(e_sample)\n",
    "                            malformed_json_preds += 1\n",
    "                            print(f\"{malformed_json_preds} malformed JSON outputs\")\n",
    "                            print(f\"Gold: {golds[idx]}\")\n",
    "                            print(f\"Prediction: {decoded_outputs[idx]}\")\n",
    "                            sorted_pred_dict = OrderedDict([('medications', [])])\n",
    "            \n",
    "                        # Write the data to the CSV file\n",
    "                        csvwriter.writerow([texts[idx], golds[idx], sorted_pred_dict])\n",
    "        \n",
    "                    # Flush the file buffer\n",
    "                    csvfile.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNSLOTH",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
